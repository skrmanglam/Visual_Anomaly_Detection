{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181d9d2f-5a40-4533-be56-16063f915d1b",
   "metadata": {},
   "source": [
    "# Visual Anomaly detection\n",
    "\n",
    "Problem Statement:\n",
    "\n",
    "Specific Libs used: Pytorch, Anomlib(Based on Ganomly)\n",
    "\n",
    "Dataset Used: Mvtech AD( Trsisostor class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a11d7239-d138-4d31-b3ed-00fb07e107db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting work directory\n",
    "import os\n",
    "\n",
    "work_dir = '/transitor/transitor'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "628c5496-51b9-43b3-8105-664f057d2566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import neccessary libs\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import yaml, warnings,math,glob, cv2, random\n",
    "import logging\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b7de801-3f28-4476-b6fb-1fe0d58e6741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the warning settings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#setting logger settings for debugging\n",
    "logger = logging.getLogger(\"anomalib\")\n",
    "\n",
    "import anomalib\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from anomalib.config import get_configurable_parameters\n",
    "from anomalib.data import get_datamodule\n",
    "from anomalib.models import get_model\n",
    "from anomalib.utils.callbacks import LoadModelCallback, get_callbacks\n",
    "from anomalib.utils.loggers import configure_logger, get_experiment_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "554d9a6c-e68d-4fd6-bd31-62fc110fb15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "11.6\n",
      "8302\n",
      "True\n",
      "1\n",
      "0\n",
      "<torch.cuda.device object at 0x000001B5EDAFB1C0>\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25a2cf1-915a-45e7-b139-3b800c137eb1",
   "metadata": {},
   "source": [
    "# Model Config Path\n",
    "\n",
    "Currently, there are **12** anomaly detection models available in `anomalib` library. Namely, \n",
    "\n",
    "- [CFA](https://arxiv.org/abs/2206.04325)\n",
    "- [Patchcore](https://arxiv.org/pdf/2106.08265.pdf)\n",
    "- [Padim](https://arxiv.org/pdf/2011.08785.pdf)\n",
    "- [DFKDE](https://github.com/openvinotoolkit/anomalib/tree/development/anomalib/models/dfkde)\n",
    "- [DFM](https://arxiv.org/pdf/1909.11786.pdf)\n",
    "- [CFlow](https://arxiv.org/pdf/2107.12571v1.pdf)\n",
    "- [Ganomaly](https://arxiv.org/abs/1805.06725)\n",
    "- [STFPM](https://arxiv.org/pdf/2103.04257.pdf)\n",
    "- [FastFlow](https://arxiv.org/abs/2111.07677)\n",
    "- [DREAM](https://arxiv.org/pdf/2108.07610v2.pdf)\n",
    "- [Reverse Distillation](https://arxiv.org/pdf/2201.10703v2.pdf)\n",
    "- [EfficientAD](https://arxiv.org/pdf/2303.14535.pdf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now, let's get their config paths from the respected folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c531c280-2099-40ce-9c93-2240ff93282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATHS = 'C:/Users/skrma/anaconda3/envs/Pytorch/Lib/site-packages/anomalib/models'\n",
    "\n",
    "MODEL_CONFIG_PAIRS = {\n",
    "    'cfa'      :   f'{CONFIG_PATHS}/cfa/config.yaml',\n",
    "    'efficientad': f'{CONFIG_PATHS}/efficientad/config.yaml',\n",
    "    'patchcore':   f'{CONFIG_PATHS}/patchcore/config.yaml',\n",
    "    'padim':       f'{CONFIG_PATHS}/padim/config.yaml',\n",
    "    'cflow':       f'{CONFIG_PATHS}/cflow/config.yaml',\n",
    "    'dfkde':       f'{CONFIG_PATHS}/dfkde/config.yaml',\n",
    "    'dfm':         f'{CONFIG_PATHS}/dfm/config.yaml',\n",
    "    'ganomaly':    f'{CONFIG_PATHS}/ganomaly/config.yaml',\n",
    "    'stfpm':       f'{CONFIG_PATHS}/stfpm/config.yaml',\n",
    "    'fastflow':    f'{CONFIG_PATHS}/fastflow/config.yaml',\n",
    "    'draem':       f'{CONFIG_PATHS}/draem/config.yaml',\n",
    "    'reverse_distillation': f'{CONFIG_PATHS}/reverse_distillation/config.yaml',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "398ada0b-7a20-480c-8001-f5d627bf9687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  name: mvtec\n",
      "  format: mvtec\n",
      "  path: ./datasets/MVTec\n",
      "  category: bottle\n",
      "  task: segmentation\n",
      "  train_batch_size: 1\n",
      "  eval_batch_size: 16\n",
      "  num_workers: 8\n",
      "  image_size: 256 # dimensions to which images are resized (mandatory)\n",
      "  center_crop: null # dimensions to which images are center-cropped after resizing (optional)\n",
      "  normalization: none # data distribution to which the images will be normalized: [none, imagenet]\n",
      "  transform_config:\n",
      "    train: null\n",
      "    eval: null\n",
      "  test_split_mode: from_dir # options: [from_dir, synthetic]\n",
      "  test_split_ratio: 0.2 # fraction of train images held out testing (usage depends on test_split_mode)\n",
      "  val_split_mode: same_as_test # options: [same_as_test, from_test, synthetic]\n",
      "  val_split_ratio: 0.5 # fraction of train/test images held out for validation (usage depends on val_split_mode)\n",
      "\n",
      "model:\n",
      "  name: efficientad\n",
      "  teacher_out_channels: 384\n",
      "  model_size: medium # options: [small, medium]\n",
      "  lr: 0.0001\n",
      "  weight_decay: 0.00001\n",
      "  padding: true\n",
      "  # generic params\n",
      "  normalization_method: min_max # options: [null, min_max, cdf]\n",
      "\n",
      "metrics:\n",
      "  image:\n",
      "    - F1Score\n",
      "    - AUROC\n",
      "  pixel:\n",
      "    - F1Score\n",
      "    - AUROC\n",
      "  threshold:\n",
      "    method: adaptive #options: [adaptive, manual]\n",
      "    manual_image: null\n",
      "    manual_pixel: null\n",
      "\n",
      "visualization:\n",
      "  show_images: False # show images on the screen\n",
      "  save_images: True # save images to the file system\n",
      "  log_images: False # log images to the available loggers (if any)\n",
      "  image_save_path: null # path to which images will be saved\n",
      "  mode: full # options: [\"full\", \"simple\"]\n",
      "\n",
      "project:\n",
      "  seed: 42\n",
      "  path: ./results\n",
      "\n",
      "logging:\n",
      "  logger: [] # options: [comet, tensorboard, wandb, csv] or combinations.\n",
      "  log_graph: false # Logs the model graph to respective logger.\n",
      "\n",
      "optimization:\n",
      "  export_mode: null # options: torch, onnx, openvino\n",
      "# PL Trainer Args. Don't add extra parameter here.\n",
      "trainer:\n",
      "  enable_checkpointing: true\n",
      "  default_root_dir: null\n",
      "  gradient_clip_val: 0\n",
      "  gradient_clip_algorithm: norm\n",
      "  num_nodes: 1\n",
      "  devices: 1\n",
      "  enable_progress_bar: true\n",
      "  overfit_batches: 0.0\n",
      "  track_grad_norm: -1\n",
      "  check_val_every_n_epoch: 1\n",
      "  fast_dev_run: false\n",
      "  accumulate_grad_batches: 1\n",
      "  max_epochs: 200\n",
      "  min_epochs: null\n",
      "  max_steps: -1\n",
      "  min_steps: null\n",
      "  max_time: null\n",
      "  limit_train_batches: 1.0\n",
      "  limit_val_batches: 1.0\n",
      "  limit_test_batches: 1.0\n",
      "  limit_predict_batches: 1.0\n",
      "  val_check_interval: 1.0\n",
      "  log_every_n_steps: 50\n",
      "  accelerator: auto # <\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\">\n",
      "  strategy: null\n",
      "  sync_batchnorm: false\n",
      "  precision: 32\n",
      "  enable_model_summary: true\n",
      "  num_sanity_val_steps: 0\n",
      "  profiler: null\n",
      "  benchmark: false\n",
      "  deterministic: false\n",
      "  reload_dataloaders_every_n_epochs: 0\n",
      "  auto_lr_find: false\n",
      "  replace_sampler_ddp: true\n",
      "  detect_anomaly: false\n",
      "  auto_scale_batch_size: false\n",
      "  plugins: null\n",
      "  move_metrics_to_cpu: false\n",
      "  multiple_trainloader_mode: max_size_cycle\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL = 'efficientad' \n",
    "\n",
    "print(open(os.path.join(MODEL_CONFIG_PAIRS[MODEL]), 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a8ea585-9f23-456d-b53a-ea3d2fb646c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update config\n",
    "\n",
    "new_update = {\n",
    "    \"path\" : '/transitor/transitor',\n",
    "    \"category\" : 'transistor',\n",
    "    \"image_size\" : 256,\n",
    "    \"train_batch_size\": 48,\n",
    "    \"seed\" : 101\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38874322-4583-429c-b84a-e7070a4acef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update yaml key's value\n",
    "\n",
    "def update_yaml (old_yaml, new_yaml, new_update):\n",
    "    # load yaml\n",
    "    with open (old_yaml) as f:\n",
    "        old = yaml.safe_load(f)\n",
    "        \n",
    "    temp = []\n",
    "    def set_state(old, key, value):\n",
    "        if isinstance(old, dict):\n",
    "            for k, v in old.items():\n",
    "                if k == 'project':\n",
    "                    temp.append(k)\n",
    "                if k == key:\n",
    "                    if temp and k == 'path':\n",
    "                        # right now not altering 'project.path' configuration\n",
    "                        continue\n",
    "                    old[k] = value\n",
    "                elif isinstance(v, dict):\n",
    "                    set_state(v, key, value)\n",
    "                    \n",
    "    # iterate over the new update key-value pair\n",
    "    for key, value in new_update.items():\n",
    "        set_state(old, key, value)\n",
    "        \n",
    "    #save the updated/ modified yaml file\n",
    "    with open(new_yaml, 'w') as f:\n",
    "        yaml.safe_dump(old, f, default_flow_style = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f111854-e089-4015-8892-2bfcc0464d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting a new path locartion of new config file\n",
    "new_yaml_path = CONFIG_PATHS + '/' + list(MODEL_CONFIG_PAIRS.keys())[0] + '_new.yaml'\n",
    "\n",
    "# run the update yaml method to update desired key's values\n",
    "update_yaml(MODEL_CONFIG_PAIRS[MODEL], new_yaml_path, new_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcd4a3db-ea86-4f02-aee2-0743b09ee5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'category': 'transistor',\n",
      "             'center_crop': None,\n",
      "             'eval_batch_size': 16,\n",
      "             'format': 'mvtec',\n",
      "             'image_size': 256,\n",
      "             'name': 'mvtec',\n",
      "             'normalization': 'none',\n",
      "             'num_workers': 8,\n",
      "             'path': '/transitor/transitor',\n",
      "             'task': 'segmentation',\n",
      "             'test_split_mode': 'from_dir',\n",
      "             'test_split_ratio': 0.2,\n",
      "             'train_batch_size': 48,\n",
      "             'transform_config': {'eval': None, 'train': None},\n",
      "             'val_split_mode': 'same_as_test',\n",
      "             'val_split_ratio': 0.5},\n",
      " 'logging': {'log_graph': False, 'logger': []},\n",
      " 'metrics': {'image': ['F1Score', 'AUROC'],\n",
      "             'pixel': ['F1Score', 'AUROC'],\n",
      "             'threshold': {'manual_image': None,\n",
      "                           'manual_pixel': None,\n",
      "                           'method': 'adaptive'}},\n",
      " 'model': {'lr': 0.0001,\n",
      "           'model_size': 'medium',\n",
      "           'name': 'efficientad',\n",
      "           'normalization_method': 'min_max',\n",
      "           'padding': True,\n",
      "           'teacher_out_channels': 384,\n",
      "           'weight_decay': 1e-05},\n",
      " 'optimization': {'export_mode': None},\n",
      " 'project': {'path': './results', 'seed': 101},\n",
      " 'trainer': {'accelerator': 'auto',\n",
      "             'accumulate_grad_batches': 1,\n",
      "             'auto_lr_find': False,\n",
      "             'auto_scale_batch_size': False,\n",
      "             'benchmark': False,\n",
      "             'check_val_every_n_epoch': 1,\n",
      "             'default_root_dir': None,\n",
      "             'detect_anomaly': False,\n",
      "             'deterministic': False,\n",
      "             'devices': 1,\n",
      "             'enable_checkpointing': True,\n",
      "             'enable_model_summary': True,\n",
      "             'enable_progress_bar': True,\n",
      "             'fast_dev_run': False,\n",
      "             'gradient_clip_algorithm': 'norm',\n",
      "             'gradient_clip_val': 0,\n",
      "             'limit_predict_batches': 1.0,\n",
      "             'limit_test_batches': 1.0,\n",
      "             'limit_train_batches': 1.0,\n",
      "             'limit_val_batches': 1.0,\n",
      "             'log_every_n_steps': 50,\n",
      "             'max_epochs': 200,\n",
      "             'max_steps': -1,\n",
      "             'max_time': None,\n",
      "             'min_epochs': None,\n",
      "             'min_steps': None,\n",
      "             'move_metrics_to_cpu': False,\n",
      "             'multiple_trainloader_mode': 'max_size_cycle',\n",
      "             'num_nodes': 1,\n",
      "             'num_sanity_val_steps': 0,\n",
      "             'overfit_batches': 0.0,\n",
      "             'plugins': None,\n",
      "             'precision': 32,\n",
      "             'profiler': None,\n",
      "             'reload_dataloaders_every_n_epochs': 0,\n",
      "             'replace_sampler_ddp': True,\n",
      "             'strategy': None,\n",
      "             'sync_batchnorm': False,\n",
      "             'track_grad_norm': -1,\n",
      "             'val_check_interval': 1.0},\n",
      " 'visualization': {'image_save_path': None,\n",
      "                   'log_images': False,\n",
      "                   'mode': 'full',\n",
      "                   'save_images': True,\n",
      "                   'show_images': False}}\n"
     ]
    }
   ],
   "source": [
    "with open(new_yaml_path) as f:\n",
    "    updated_config = yaml.safe_load(f)\n",
    "pprint.pprint(updated_config) # check if upadted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df9b77-c2e9-45b8-ae75-7ac90d88506d",
   "metadata": {},
   "source": [
    "### Prepare Model, Dataloader, Callback with config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e23c86c-aae2-4910-b8b3-cf070d770167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "if updated_config['project']['seed'] != 0:\n",
    "    print(updated_config['project']['seed'])\n",
    "    seed_everything(updated_config['project']['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "476dc295-44f6-4345-8ffb-4a6bcda4b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will return the configurable parameters in DictConfig object.\n",
    "config = get_configurable_parameters(\n",
    "    model_name = updated_config['model']['name'],\n",
    "    config_path = new_yaml_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4b3c15c-9fd3-471d-a8c3-90467a67d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the config file to model, logger , callbacks and datamodule\n",
    "model = get_model(config)\n",
    "experiment_logger = get_experiment_logger(config)\n",
    "callbacks = get_callbacks(config)\n",
    "datamodule = get_datamodule(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b7876db-7273-4cfb-b307-ddd1428f95d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(limit_predict_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                  | Type                     | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | image_threshold       | AnomalyScoreThreshold    | 0     \n",
      "1 | pixel_threshold       | AnomalyScoreThreshold    | 0     \n",
      "2 | model                 | EfficientADModel         | 20.7 M\n",
      "3 | image_metrics         | AnomalibMetricCollection | 0     \n",
      "4 | pixel_metrics         | AnomalibMetricCollection | 0     \n",
      "5 | normalization_metrics | MinMax                   | 0     \n",
      "-------------------------------------------------------------------\n",
      "20.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "20.7 M    Total params\n",
      "82.957    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45dc5d7664a246a1820025efc1237d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate teacher channel mean:   0%|                                                            | 0/5 [00:30<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.07 GiB (GPU 0; 6.00 GiB total capacity; 3.22 GiB already allocated; 503.00 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtrainer, logger\u001b[38;5;241m=\u001b[39mexperiment_logger, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    606\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 608\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     41\u001b[0m     trainer\u001b[38;5;241m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    643\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_set_ckpt_path(\n\u001b[0;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    646\u001b[0m     ckpt_path,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    647\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    648\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    649\u001b[0m )\n\u001b[1;32m--> 650\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[1;32m-> 1112\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1114\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1191\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[0;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1214\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1214\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:194\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_skip()\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m--> 194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_run_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:218\u001b[0m, in \u001b[0;36mFitLoop.on_run_start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 218\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_start\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_call_strategy_hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1356\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[1;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1353\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[0;32m   1355\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1356\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1359\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\anomalib\\models\\efficientad\\lightning_model.py:196\u001b[0m, in \u001b[0;36mEfficientAD.on_train_start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m\"\"\"Calculate or load the channel-wise mean and std of the training dataset and push to the model.\"\"\"\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mis_set(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmean_std):\n\u001b[1;32m--> 196\u001b[0m     channel_mean_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mteacher_channel_mean_std\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmean_std\u001b[38;5;241m.\u001b[39mupdate(channel_mean_std)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\anomalib\\models\\efficientad\\lightning_model.py:137\u001b[0m, in \u001b[0;36mEfficientAD.teacher_channel_mean_std\u001b[1;34m(self, dataloader)\u001b[0m\n\u001b[0;32m    135\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculate teacher channel mean and std\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculate teacher channel mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 137\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mteacher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m     y_means\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mmean(y, dim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]))\n\u001b[0;32m    139\u001b[0m     teacher_outputs\u001b[38;5;241m.\u001b[39mappend(y)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\anomalib\\models\\efficientad\\torch_model.py:83\u001b[0m, in \u001b[0;36mPDN_M.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     82\u001b[0m     x \u001b[38;5;241m=\u001b[39m imagenet_norm_batch(x)\n\u001b[1;32m---> 83\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool1(x)\n\u001b[0;32m     85\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.07 GiB (GPU 0; 6.00 GiB total capacity; 3.22 GiB already allocated; 503.00 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "torch.cuda.empty_cache()\n",
    "trainer = Trainer(**config.trainer, logger=experiment_logger, callbacks=callbacks)\n",
    "trainer.fit(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf359e7-b117-46b5-89dd-4bc4a2829c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
